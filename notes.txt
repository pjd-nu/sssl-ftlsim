10/17/2012
two-level hot/cold, p=0.1 alpha=1.1 r=0.8 f=0.2
at cleaning time:
  level 1 hot=0.5153 cold=0.1947 invalid=0.2901
  level 2 hot=0.0829 cold=0.7650 invalid=0.1520
overall stats (f is fraction total valid pages):
  level 1 valid=0.8446 invalid=0.1554 alpha=1.1840 f=0.0845
          hot pages = 0.0712 cold pages = 0.0217
  level 2 valid=0.9163 invalid=0.0837 alpha=1.0914 f=0.8246
  	  hot pages = 0.1288 cold pages = 0.7783

Looking at the 2nd stage as a hot/cold LRU, we have:
  alpha=1.0914
  r = 0.5153/(0.5153+0.1947) = 0.7258
  f = 0.1288 / (0.1288+0.7783) = 0.1420
  lru_hc gives 6.5900, vs. 6.59038 from the calculation in two_pool.

4/25/2012
test.sh:
    for f in 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.3 2.6 2.9 4.2 4.5 5.0 5.5 6.0 6.5 7.0 8 9 10; do
	echo fffff $f
	python multi-pool.py $f
    done
(actually single LRU pool w/ 128 and 16-page blocks)
process:
awk '(/ffff/){nn=$2}/^[0-9]/{if (NF==3){print nn,$3}}' typescript  | dbcoldefine f A | dbmultistats --key=f A | dbcol f mean conf_low conf_high | dbsort -n f > data.dat

4/20/2012
moved cleaning selection into Python - 2.6s

4/18/2012
runlru - U = 23020, Np = 128, random, S_f = 0.2, running U*Np steps:
  simple python code - 275s [runlru.py]
  moved int_write() and host_write() into C - 32.4 s [runlru2.py]
  used C source for uniform, run() - 6.67 s [runlru3]
  using C source for writes, warmup - 2.37 s [runlru4]
  old greedy.c - 2s

4/17/2012 - python code is awfully slow.
Fin1 trace (4M lines, 5.6M writes):
 9.9s pypy
 28.1s python
to just iterate over all the addresses.

simple C parser for trace file:
 7.8s python (SWIG doesn't work with pypy)

random source, 5.6M addresses:
 29.9s python
 1.1s pypy
 28.2s - python w/ iterator
 5.5s - C SWIG version


